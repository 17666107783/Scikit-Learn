{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 9 – Up and running with TensorFlow**\n",
    "_This notebook contains all the sample code and solutions to the exercises in chapter 9._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()#用于清除默认图形堆栈，重置全局默认图。\n",
    "\n",
    "x=tf.Variable(3,name=\"x\")\n",
    "y=tf.Variable(4,name=\"y\")\n",
    "f=x*x*y+y+2\n",
    "f\n",
    "#add冒号后面的数字编号表示这个张量是计算节点上的第几个结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result=sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在with块，会有一个默认块\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result=f.eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#调用初始化器进行初始化\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()           #此时初始化\n",
    "    result=f.eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#InteractiveSession 在创建时会将自己设置为默认会话\n",
    "sess=tf.InteractiveSession()\n",
    "init.run()\n",
    "result=f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建一个新图，用with块临时设为默认图\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2=tf.Variable(2)\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#节点值得生命周期\n",
    "w=tf.constant(3)\n",
    "x=w+2\n",
    "y=x+5\n",
    "z=x*3\n",
    "\n",
    "with tf.Session() as sess:#运行两次\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val,z_val=sess.run([y,z])#只运行一次\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Using the Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  立刻下载数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing=fetch_california_housing(data_home=\"datasets/mldata\", download_if_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "(20640,)\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(housing.data.shape)\n",
    "print(type(housing.data))\n",
    "print(housing.target.shape)\n",
    "print(housing.feature_names)\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_read_csv()把第一行的数据变成了列名,这个csv的第一行并不是我们想象中的那样是一个列名。那样，我们处理数据的时候，就会出现问题，第一个不一致了嘛。\n",
    "解决方法是设置参数！例如：df = pd.read_csv('1.csv', header=None, Names=['test']) _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>269700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2535.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>3.6591</td>\n",
       "      <td>299200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3104.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>3.1200</td>\n",
       "      <td>241400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.84</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>2.0804</td>\n",
       "      <td>226700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3549.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>3.6912</td>\n",
       "      <td>261100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>3.2031</td>\n",
       "      <td>281500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3503.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>3.2705</td>\n",
       "      <td>241800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2491.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>3.0750</td>\n",
       "      <td>213500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2.6736</td>\n",
       "      <td>191300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2643.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>1.9167</td>\n",
       "      <td>159200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.85</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>140000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>2.7750</td>\n",
       "      <td>152500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2.1202</td>\n",
       "      <td>155500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.84</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2239.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>1.9911</td>\n",
       "      <td>158700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1503.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>2.6033</td>\n",
       "      <td>162900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.85</td>\n",
       "      <td>40.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.3578</td>\n",
       "      <td>147500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.85</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1639.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>1.7135</td>\n",
       "      <td>159800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>113900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1688.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>2.1806</td>\n",
       "      <td>99700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>2.6000</td>\n",
       "      <td>132600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-122.28</td>\n",
       "      <td>37.85</td>\n",
       "      <td>41.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2.4038</td>\n",
       "      <td>107500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-122.28</td>\n",
       "      <td>37.85</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>2.4597</td>\n",
       "      <td>93800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-122.28</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>1.8080</td>\n",
       "      <td>105500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-122.28</td>\n",
       "      <td>37.84</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2082.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>1.6424</td>\n",
       "      <td>108900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-122.28</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.6875</td>\n",
       "      <td>132000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20610</th>\n",
       "      <td>-121.56</td>\n",
       "      <td>39.10</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>1.3631</td>\n",
       "      <td>45500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20611</th>\n",
       "      <td>-121.55</td>\n",
       "      <td>39.10</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1783.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1.2857</td>\n",
       "      <td>47000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20612</th>\n",
       "      <td>-121.56</td>\n",
       "      <td>39.08</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1377.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>761.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1.4934</td>\n",
       "      <td>48300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20613</th>\n",
       "      <td>-121.55</td>\n",
       "      <td>39.09</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1728.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>1.4958</td>\n",
       "      <td>53400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20614</th>\n",
       "      <td>-121.54</td>\n",
       "      <td>39.08</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2276.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>2.4695</td>\n",
       "      <td>58000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20615</th>\n",
       "      <td>-121.54</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>2.3598</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20616</th>\n",
       "      <td>-121.53</td>\n",
       "      <td>39.08</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>2.0469</td>\n",
       "      <td>55100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20617</th>\n",
       "      <td>-121.53</td>\n",
       "      <td>39.06</td>\n",
       "      <td>20.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.3021</td>\n",
       "      <td>70800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20618</th>\n",
       "      <td>-121.55</td>\n",
       "      <td>39.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>2.2500</td>\n",
       "      <td>63400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20619</th>\n",
       "      <td>-121.56</td>\n",
       "      <td>39.01</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1891.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>2.7303</td>\n",
       "      <td>99100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20620</th>\n",
       "      <td>-121.48</td>\n",
       "      <td>39.05</td>\n",
       "      <td>40.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.5625</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20621</th>\n",
       "      <td>-121.47</td>\n",
       "      <td>39.01</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>2.3661</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20622</th>\n",
       "      <td>-121.44</td>\n",
       "      <td>39.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>2.4167</td>\n",
       "      <td>67000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20623</th>\n",
       "      <td>-121.37</td>\n",
       "      <td>39.03</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>2.8235</td>\n",
       "      <td>65500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20624</th>\n",
       "      <td>-121.41</td>\n",
       "      <td>39.04</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1698.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>3.0739</td>\n",
       "      <td>87200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20625</th>\n",
       "      <td>-121.52</td>\n",
       "      <td>39.12</td>\n",
       "      <td>37.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.1250</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>-121.43</td>\n",
       "      <td>39.18</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>2.1667</td>\n",
       "      <td>93800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>162500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>-121.48</td>\n",
       "      <td>39.10</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2043.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.5952</td>\n",
       "      <td>92400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>-121.39</td>\n",
       "      <td>39.12</td>\n",
       "      <td>28.0</td>\n",
       "      <td>10035.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>6912.0</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>2.0943</td>\n",
       "      <td>108300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.29</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>3.5673</td>\n",
       "      <td>112000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>-121.40</td>\n",
       "      <td>39.33</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3.5179</td>\n",
       "      <td>107200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20632</th>\n",
       "      <td>-121.45</td>\n",
       "      <td>39.26</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2319.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>3.1250</td>\n",
       "      <td>115600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20633</th>\n",
       "      <td>-121.53</td>\n",
       "      <td>39.19</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>2.5495</td>\n",
       "      <td>98300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20634</th>\n",
       "      <td>-121.56</td>\n",
       "      <td>39.27</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2332.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.7125</td>\n",
       "      <td>116800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-121.09</td>\n",
       "      <td>39.48</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>78100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-121.21</td>\n",
       "      <td>39.49</td>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>77100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-121.22</td>\n",
       "      <td>39.43</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>92300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.43</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>84700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-121.24</td>\n",
       "      <td>39.37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>89400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1     2        3       4       5       6       7         8\n",
       "0     -122.23  37.88  41.0    880.0   129.0   322.0   126.0  8.3252  452600.0\n",
       "1     -122.22  37.86  21.0   7099.0  1106.0  2401.0  1138.0  8.3014  358500.0\n",
       "2     -122.24  37.85  52.0   1467.0   190.0   496.0   177.0  7.2574  352100.0\n",
       "3     -122.25  37.85  52.0   1274.0   235.0   558.0   219.0  5.6431  341300.0\n",
       "4     -122.25  37.85  52.0   1627.0   280.0   565.0   259.0  3.8462  342200.0\n",
       "5     -122.25  37.85  52.0    919.0   213.0   413.0   193.0  4.0368  269700.0\n",
       "6     -122.25  37.84  52.0   2535.0   489.0  1094.0   514.0  3.6591  299200.0\n",
       "7     -122.25  37.84  52.0   3104.0   687.0  1157.0   647.0  3.1200  241400.0\n",
       "8     -122.26  37.84  42.0   2555.0   665.0  1206.0   595.0  2.0804  226700.0\n",
       "9     -122.25  37.84  52.0   3549.0   707.0  1551.0   714.0  3.6912  261100.0\n",
       "10    -122.26  37.85  52.0   2202.0   434.0   910.0   402.0  3.2031  281500.0\n",
       "11    -122.26  37.85  52.0   3503.0   752.0  1504.0   734.0  3.2705  241800.0\n",
       "12    -122.26  37.85  52.0   2491.0   474.0  1098.0   468.0  3.0750  213500.0\n",
       "13    -122.26  37.84  52.0    696.0   191.0   345.0   174.0  2.6736  191300.0\n",
       "14    -122.26  37.85  52.0   2643.0   626.0  1212.0   620.0  1.9167  159200.0\n",
       "15    -122.26  37.85  50.0   1120.0   283.0   697.0   264.0  2.1250  140000.0\n",
       "16    -122.27  37.85  52.0   1966.0   347.0   793.0   331.0  2.7750  152500.0\n",
       "17    -122.27  37.85  52.0   1228.0   293.0   648.0   303.0  2.1202  155500.0\n",
       "18    -122.26  37.84  50.0   2239.0   455.0   990.0   419.0  1.9911  158700.0\n",
       "19    -122.27  37.84  52.0   1503.0   298.0   690.0   275.0  2.6033  162900.0\n",
       "20    -122.27  37.85  40.0    751.0   184.0   409.0   166.0  1.3578  147500.0\n",
       "21    -122.27  37.85  42.0   1639.0   367.0   929.0   366.0  1.7135  159800.0\n",
       "22    -122.27  37.84  52.0   2436.0   541.0  1015.0   478.0  1.7250  113900.0\n",
       "23    -122.27  37.84  52.0   1688.0   337.0   853.0   325.0  2.1806   99700.0\n",
       "24    -122.27  37.84  52.0   2224.0   437.0  1006.0   422.0  2.6000  132600.0\n",
       "25    -122.28  37.85  41.0    535.0   123.0   317.0   119.0  2.4038  107500.0\n",
       "26    -122.28  37.85  49.0   1130.0   244.0   607.0   239.0  2.4597   93800.0\n",
       "27    -122.28  37.85  52.0   1898.0   421.0  1102.0   397.0  1.8080  105500.0\n",
       "28    -122.28  37.84  50.0   2082.0   492.0  1131.0   473.0  1.6424  108900.0\n",
       "29    -122.28  37.84  52.0    729.0   160.0   395.0   155.0  1.6875  132000.0\n",
       "...       ...    ...   ...      ...     ...     ...     ...     ...       ...\n",
       "20610 -121.56  39.10  28.0   2130.0   484.0  1195.0   439.0  1.3631   45500.0\n",
       "20611 -121.55  39.10  27.0   1783.0   441.0  1163.0   409.0  1.2857   47000.0\n",
       "20612 -121.56  39.08  26.0   1377.0   289.0   761.0   267.0  1.4934   48300.0\n",
       "20613 -121.55  39.09  31.0   1728.0   365.0  1167.0   384.0  1.4958   53400.0\n",
       "20614 -121.54  39.08  26.0   2276.0   460.0  1455.0   474.0  2.4695   58000.0\n",
       "20615 -121.54  39.08  23.0   1076.0   216.0   724.0   197.0  2.3598   57500.0\n",
       "20616 -121.53  39.08  15.0   1810.0   441.0  1157.0   375.0  2.0469   55100.0\n",
       "20617 -121.53  39.06  20.0    561.0   109.0   308.0   114.0  3.3021   70800.0\n",
       "20618 -121.55  39.06  25.0   1332.0   247.0   726.0   226.0  2.2500   63400.0\n",
       "20619 -121.56  39.01  22.0   1891.0   340.0  1023.0   296.0  2.7303   99100.0\n",
       "20620 -121.48  39.05  40.0    198.0    41.0   151.0    48.0  4.5625  100000.0\n",
       "20621 -121.47  39.01  37.0   1244.0   247.0   484.0   157.0  2.3661   77500.0\n",
       "20622 -121.44  39.00  20.0    755.0   147.0   457.0   157.0  2.4167   67000.0\n",
       "20623 -121.37  39.03  32.0   1158.0   244.0   598.0   227.0  2.8235   65500.0\n",
       "20624 -121.41  39.04  16.0   1698.0   300.0   731.0   291.0  3.0739   87200.0\n",
       "20625 -121.52  39.12  37.0    102.0    17.0    29.0    14.0  4.1250   72000.0\n",
       "20626 -121.43  39.18  36.0   1124.0   184.0   504.0   171.0  2.1667   93800.0\n",
       "20627 -121.32  39.13   5.0    358.0    65.0   169.0    59.0  3.0000  162500.0\n",
       "20628 -121.48  39.10  19.0   2043.0   421.0  1018.0   390.0  2.5952   92400.0\n",
       "20629 -121.39  39.12  28.0  10035.0  1856.0  6912.0  1818.0  2.0943  108300.0\n",
       "20630 -121.32  39.29  11.0   2640.0   505.0  1257.0   445.0  3.5673  112000.0\n",
       "20631 -121.40  39.33  15.0   2655.0   493.0  1200.0   432.0  3.5179  107200.0\n",
       "20632 -121.45  39.26  15.0   2319.0   416.0  1047.0   385.0  3.1250  115600.0\n",
       "20633 -121.53  39.19  27.0   2080.0   412.0  1082.0   382.0  2.5495   98300.0\n",
       "20634 -121.56  39.27  28.0   2332.0   395.0  1041.0   344.0  3.7125  116800.0\n",
       "20635 -121.09  39.48  25.0   1665.0   374.0   845.0   330.0  1.5603   78100.0\n",
       "20636 -121.21  39.49  18.0    697.0   150.0   356.0   114.0  2.5568   77100.0\n",
       "20637 -121.22  39.43  17.0   2254.0   485.0  1007.0   433.0  1.7000   92300.0\n",
       "20638 -121.32  39.43  18.0   1860.0   409.0   741.0   349.0  1.8672   84700.0\n",
       "20639 -121.24  39.37  16.0   2785.0   616.0  1387.0   530.0  2.3886   89400.0\n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#直接读取t下载好的文件\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"mldata\")\n",
    "import pandas as pd\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    path = os.path.join(housing_path, \"CaliforniaHousing\")\n",
    "    csv_path = os.path.join(path, \"cal_housing.data\")\n",
    "    return pd.read_csv(csv_path,header=None)           #header=None,\n",
    "house = load_housing_data()              #都出来的数据，它把第一行的数据当成了索引，忽略掉了\n",
    "house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#csv函数读取的数据\n",
    "house_data = house.iloc[:, :-1]#[:-1]就是去除了的最后一列后剩下的部分。iloc根据位置索引\n",
    "house_target = house.iloc[:, -1] #最后一列,返回的值有索引,如果只是获取一行数据的话，返回Serie对象\n",
    "print(house_data.shape)\n",
    "print(house_target.shape)\n",
    "print(type(house_data))        #DataFrame相当于有表格，有行表头和列表头\n",
    "print(type(house_target))      #Series 它是有索引，如果我们未指定索引，则是以数字自动生成，Series的俩个属性values和index获取内容和索引:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n",
      "[ 4.526  3.585  3.521 ...,  0.923  0.847  0.894]\n"
     ]
    }
   ],
   "source": [
    "# housing=fetch_california_housing得到的数据\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)\n",
    "print(housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n",
      "0   -122.2300\n",
      "1     37.8800\n",
      "2     41.0000\n",
      "3    880.0000\n",
      "4    129.0000\n",
      "5    322.0000\n",
      "6    126.0000\n",
      "7      8.3252\n",
      "Name: 0, dtype: float64\n",
      "<class 'numpy.ndarray'>\n",
      "[-122.23     37.88     41.      880.      129.      322.      126.\n",
      "    8.3252]\n"
     ]
    }
   ],
   "source": [
    "print(housing.data[0,:])#不知道为什么housing.data和house中的数据有差距？？？？？\n",
    "print(house_data.loc[0])    #Series对象\n",
    "print(type(house_data.values))     #DataFrame转ndarray\n",
    "print(house_data.values[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用house数据进行数据处理线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,n=house_data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), house_data]\n",
    "housing_data_plus_bias[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640,)\n",
      "RangeIndex(start=0, stop=20640, step=1)\n",
      "[ 452600.  358500.  352100. ...,   92300.   84700.   89400.]\n"
     ]
    }
   ],
   "source": [
    "#  Series的俩个属性values和index获取内容和索引:\n",
    "#  Series对象（属于pandas库），不能直接用reshape函数(属于numpy库)， house_target.values.reshape(-1, 1)\n",
    "print(house_target.shape)\n",
    "print(house_target.index)\n",
    "print(house_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.53207150e+06]\n",
      " [ -4.21040938e+04]\n",
      " [ -4.19390430e+04]\n",
      " [  1.16819812e+03]\n",
      " [ -8.33691406e+00]\n",
      " [  1.13105164e+02]\n",
      " [ -3.85289040e+01]\n",
      " [  4.95863571e+01]\n",
      " [  4.03551016e+04]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")  \n",
    "y = tf.constant(house_target.values.reshape(-1, 1), dtype=tf.float32, name=\"y\")  #reshape(-1, 1)变成一列\n",
    "XT = tf.transpose(X)  #转置\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)  \n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    theta_value = theta.eval()  \n",
    "print(theta_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用housing数据进行线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.64127045e+01],\n",
       "       [  4.37925577e-01],\n",
       "       [  9.52987000e-03],\n",
       "       [ -1.08451903e-01],\n",
       "       [  6.48505449e-01],\n",
       "       [ -3.66319728e-06],\n",
       "       [ -3.80029809e-03],\n",
       "       [ -4.15675610e-01],\n",
       "       [ -4.28362131e-01]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()#用于清除默认图形堆栈，重置全局默认图。\n",
    "m, n = housing.data.shape\n",
    "# 这里添加一个额外的bias输入特征(x0=1)到所有的训练数据上面，因为使用的numpy所有会立即执行\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "# 创建两个TensorFlow常量节点X和y，持有数据和标签\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)   #转置成列向量\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)  #使用normal equation的方法求解theta，之前线性模型中有提及\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare with pure NumPy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compare with  Scikit-Learn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])#截距，斜率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Batch Gradient Descent实现梯度下降\n",
    "当使用梯度下降时，请记住，首先要对输入特征向量进行归一化，否则训练可能要慢得多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "## StandardScaler默认就做了方差归一化，和均值归一化，这两个归一化的目的都是为了更快的进行梯度下降\n",
    "scaler=StandardScaler()\n",
    "scaled_housing_data=scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)),scaled_housing_data]\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients手动计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 8.90509\n",
      "Epoch 100 MSE= 0.704832\n",
      "Epoch 200 MSE= 0.575422\n",
      "Epoch 300 MSE= 0.561636\n",
      "Epoch 400 MSE= 0.553213\n",
      "Epoch 500 MSE= 0.54687\n",
      "Epoch 600 MSE= 0.542014\n",
      "Epoch 700 MSE= 0.538272\n",
      "Epoch 800 MSE= 0.535375\n",
      "Epoch 900 MSE= 0.53312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855226],\n",
       "       [ 0.88492042],\n",
       "       [ 0.14619108],\n",
       "       [-0.33808011],\n",
       "       [ 0.35164922],\n",
       "       [ 0.00476077],\n",
       "       [-0.04282646],\n",
       "       [-0.64792496],\n",
       "       [-0.62347209]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "## random_uniform函数创建图里一个节点包含随机数值，给定它的形状和取值范围\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1,1),name='theta')   #使用gradient需要有一个初值\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")   #x是m*（n+1），theta是（n+1）*1\n",
    "error=y_pred-y\n",
    "#TensorFlow求解均值功能强大\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "# 梯度的公式：(y_pred - y) * xj\n",
    "#自己写训练过程，实际可以采用TensorFlow自带的功能更强大的自动求解autodiff方法\n",
    "gradients=2/m*tf.matmul(tf.transpose(X),error)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()      #等价 sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "        training_op.eval()  #等价 sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff自动微分\n",
    "Same as above except for the gradients = ... line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 7.75823\n",
      "Epoch 100 MSE= 0.819722\n",
      "Epoch 200 MSE= 0.670463\n",
      "Epoch 300 MSE= 0.632047\n",
      "Epoch 400 MSE= 0.604961\n",
      "Epoch 500 MSE= 0.584951\n",
      "Epoch 600 MSE= 0.570107\n",
      "Epoch 700 MSE= 0.559058\n",
      "Epoch 800 MSE= 0.550802\n",
      "Epoch 900 MSE= 0.544609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.87316912],\n",
       "       [ 0.16255787],\n",
       "       [-0.28114563],\n",
       "       [ 0.28929538],\n",
       "       [ 0.01086778],\n",
       "       [-0.04396051],\n",
       "       [-0.53965646],\n",
       "       [-0.51215559]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "## random_uniform函数创建图里一个节点包含随机数值，给定它的形状和取值范围\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1,1),name='theta')   #使用gradient需要有一个初值\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")   #x是m*（n+1），theta是（n+1）*1\n",
    "error=y_pred-y\n",
    "#TensorFlow求解均值功能强大\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "# 梯度的公式：(y_pred - y) * x\n",
    "\n",
    "\n",
    "\n",
    "#采用TensorFlow自带的功能更强大的自动求解autodiff方法\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "\n",
    "\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()      #等价 sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "        training_op.eval()  #等价 sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`使用优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 MSE= 2.75443\n",
      "epoch 100 MSE= nan\n",
      "epoch 200 MSE= nan\n",
      "epoch 300 MSE= nan\n",
      "epoch 400 MSE= nan\n",
      "epoch 500 MSE= nan\n",
      "epoch 600 MSE= nan\n",
      "epoch 700 MSE= nan\n",
      "epoch 800 MSE= nan\n",
      "epoch 900 MSE= nan\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#也可以使用更快的动量优化器\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            print(\"epoch\",epoch,\"MSE=\",mse.eval())\n",
    "        #training_op.eval()   #'Operation' object has no attribute 'eval'可能是因为它没有输出\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent\n",
    "为了实现小批量梯度下降（Mini-batch Gradient Descent）。我们需要一种在每次迭代时用下一个小批量替换X和Y的方法。 最简单的方法是使用占位符（placeholder）节点\n",
    "### Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_epochs = 10  \n",
    "learning_rate = 0.01\n",
    "#批次的大小\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "#定义占位符\n",
    "X=tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform((n+1,1),-1,1,seed=42),name=\"theta\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"MSE\")\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    np.random.seed(epoch*n_batches+batch_index)\n",
    "    indices=np.random.randint(m,size=batch_size)#从0到m中，随机取出100个索引\n",
    "    X_batch=scaled_housing_data_plus_bias[indices]\n",
    "    y_batch=housing.target.reshape(-1,1)[indices]\n",
    "    return X_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver({\"weights\": theta})   #在构造阶段结束（创建所有变量节点之后）创建一个保存节点\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(epoch,batch_index,batch_size)\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "    best_theta=theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")    #调用保存方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07001591],\n",
       "       [ 0.82045609],\n",
       "       [ 0.11731728],\n",
       "       [-0.22739054],\n",
       "       [ 0.31134021],\n",
       "       [ 0.00353193],\n",
       "       [-0.01126994],\n",
       "       [-0.91643941],\n",
       "       [-0.87950087]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "#恢复模型：在构建阶段结束时创建一个保存器，就像之前一样，在执行阶段的开始，不用init节点初始化变量，调用restore()方法的保存器对象\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()        # 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)#结果完全相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#放在构造期的最后\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes命名作用域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta: [[ 2.06999016]\n",
      " [ 0.81566656]\n",
      " [ 0.11626159]\n",
      " [-0.21896598]\n",
      " [ 0.30451947]\n",
      " [ 0.00324846]\n",
      " [-0.01099628]\n",
      " [-0.92937011]\n",
      " [-0.89179307]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "\n",
    "learning_rate=0.01\n",
    "n_epochs=10\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "X=tf.placeholder(dtype=tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(dtype=tf.float32,shape=(None,1),name=\"y\")\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1,1),name=\"theta\")\n",
    "y_pred=tf.matmul(X,theta)\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:#命名作用域\n",
    "    error=y_pred-y\n",
    "    mse=tf.reduce_mean(tf.square(error),name=\"MSE\")\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)    \n",
    "training_op=optimizer.minimize(mse)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "mse_summary=tf.summary.scalar('MSE',mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str=mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                step=epoch*n_batches\n",
    "                file_writer.add_summary(summary_str,step)   #将训练过程数据保存在filewriter指定的文件中\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\",best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
