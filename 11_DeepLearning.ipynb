{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 11 – Deep Learning**\n",
    "\n",
    "_This notebook contains all the sample code and solutions to the exercises in chapter 11._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -0.2, 1.2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSggiIgLA/wPTsCHUWqFe4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ff0129f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logit(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "z=np.linspace(-5,5,200)\n",
    "plt.plot([-5,5],[0,0],'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot(z,logit(z),'b-',linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  The `dense()` function is almost identical to the `fully_connected()` function. The main differences relevant to this chapter are:\n",
    "* several parameters are renamed: `scope` becomes `name`, `activation_fn` becomes `activation` (and similarly the `_fn` suffix is removed from other parameters such as `normalizer_fn`), `weights_initializer` becomes `kernel_initializer`, etc.\n",
    "* the default `activation` is now `None` rather than `tf.nn.relu`.\n",
    "* it does not support `tensorflow.contrib.framework.arg_scope()` (introduced later in chapter 11).\n",
    "* it does not support regularizer params (introduced later in chapter 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "n_inputs=28*28\n",
    "n_hidden1=300\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_inputs),name=\"X\")\n",
    "\n",
    "he_init=tf.variance_scaling_initializer()\n",
    "hidden1=tf.layers.dense(X,n_hidden1,activation=tf.nn.relu,kernel_initializer=he_init,name=\"hidden1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions非饱和激活函数\n",
    "### 1、Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDzlgjaBsjpJfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ff0169780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def leaky_relu(z,alpha=0.01):\n",
    "    return np.maximum(alpha*z,z)\n",
    "\n",
    "z=np.linspace(-5,5,200)\n",
    "plt.plot(z,leaky_relu(z,0.05))\n",
    "plt.plot([-5,5],[0,0],'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Leaky ReLU in TensorFlow:\n",
    "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)       #注意这里的maximum函数是tensorflow库里的\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)#根据logits计算交叉熵，返回每个实例（0到9）的交叉熵的一维张量\n",
    "    loss=tf.reduce_mean(xentropy,name=\"loss\")\n",
    "\n",
    "learning_rate=0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op=optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct=tf.nn.in_top_k(logits,y,1)#检查最高的logit值对应类别是否正确，返回一维张量，bool类型\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))# 转化为浮点型，求平均\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_summary=tf.summary.scalar(\"Loss\",loss)#创建一个节点来求loss\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train,y_train),(X_test,y_test)=tf.keras.datasets.mnist.load_data()\n",
    "X_train=X_train.astype(np.float32).reshape(-1,28*28)/255.0\n",
    "X_test=X_test.astype(np.float32).reshape(-1,28*28)/255.0\n",
    "y_atrain=y_train.astype(np.int32)\n",
    "y_test=y_test.astype(np.int32)\n",
    "X_valid,X_train=X_train[:5000],X_train[5000:]\n",
    "y_valid,y_train=y_train[:5000],y_train[5000:]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_batch(X,y,batch_size):\n",
    "    rnd_idx=np.random.permutation(len(X))\n",
    "    n_batches=len(X)//batch_size#向下取整\n",
    "    for batch_idx in np.array_split(rnd_idx,n_batches):\n",
    "        X_batch,y_batch=X[batch_idx],y[batch_idx]\n",
    "        yield X_batch,y_batch       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.06 Validation accuracy: 0.06\n",
      "3 Batch accuracy: 0.08 Validation accuracy: 0.08\n",
      "6 Batch accuracy: 0.12 Validation accuracy: 0.12\n",
      "9 Batch accuracy: 0.04 Validation accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "n_epochs=10\n",
    "batch_size=50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch,y_batch in shuffle_batch(X_train,y_train,batch_size):  \n",
    "            summary_str = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            #sess.run(train_op,feed_dict={X:X_batch,y:y_batch})\n",
    "        if epoch%3==0:\n",
    "            acc_batch=accuracy.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "            acc_valid=accuracy.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "            \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ELU加速线性单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -2.2, 3.2]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEOCAYAAABsJGdEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFNW9//H3dxYX9tVxQSW4RUwUhZirUZkoUdzXq3Eh\nQRNRwCjEJWrQayLBn0avJMYQSTQoShTFKBBNblxaXDEQUUGFgOwgq804wzADPef3x+lhhtmX6qnu\n6s/reeqhp09N1bfP1HyoOX26ypxziIhINOWEXYCIiKSOQl5EJMIU8iIiEaaQFxGJMIW8iEiEKeRF\nRCJMIS8iEmEKeRGRCFPIS2DMbJKZzYzQfnLM7BEz22RmzswKU73PBmppk9ec3FdXM1tnZge1xf6a\ny8yeNbMbw64jU5g+8RoOM5sE/LCOptnOuf9Ktvdwzp1Vz/fHgPnOuetqPD8U+J1zrkOgBTdt353x\nx1Q8k/bTwP7PAp4HCoHPgc3OufJU7jO53xg1Xndbvebkvn6NP/auTPW+6tj3ScBNQH9gX+BK59yk\nGut8E3gD+Jpzbktb15hp8sIuIMu9Agyp8VzKQyRV2uoXrg1/sQ8G1jrn3mmj/dWrrV6zmbUDfgyc\n3Rb7q0MHYD7wRHKpxTn3sZl9DlwBPNyGtWUkDdeEq8w590WNZXOqd2pmg83sTTP70sw2m9k/zOzw\nau1mZjea2X/MrMzMVpnZPcm2ScBAYGRyCMOZWe/KNjObaWbDkn/u59bY7xQzm96UOpqyn2rb2d3M\nxif3uc3M3jOzE6q1x8zs92Y2zsw2mtl6M7vfzOo9/pP7fxA4ILnvZdW29bua61bW05R9taR/m/ua\nW/q6gTMAB7xdR5/0N7NXzazUzBab2UlmdrGZ1Vq3pZxzLznnbnfOPQdUNLDqdODSoPYbZQr57NQe\nGA8cix+K2ALMMLPdku3jgDuAe4C+wAXAimTbDcC7wJ+BfZLLyhrbfxboDHyv8gkz6wCcCzzZxDqa\nsp9K9wGXAFcBRwMfA383s32qrXM5sAM4HrgOGJX8nvrcAPwSWJXc97caWLemxvbV2v6Fpr3mptRS\n04nAXFdjHNfMvgW8CbwOHAm8B/wC+HnytVBj/dvNrLiR5cQG6mjM+8CxZrZnK7aRHZxzWkJYgEn4\nX77iGsu91dpnNvD9MfzYe83nhwLFzaylPZAATsD/ubwNuLYF+95ZM34se3K1tivwIb5HU+poxn7a\n44e4flCtPRdYAoyttp13a2zjn8CfGumXm4Bljb32GvU0uK+W9m9zX3NLXzfwAvB4Hc/PAp6p9vUZ\nyZ/V6/Vspxt+uKuhZc9G+r8YGFpP25H4vzgOas6xno2LxuTDNQsYVuO5tnhj7SDgbuDbQE/8X3Q5\nwAH48NgdeLWVu3kSeNzM2jnntuLPKKc557Y1sY6mOgjIp9rwgnMuYWbv4s+SK31U4/vWAHs1Yz/N\n0dC++tL6/m3qa26slrrsCayr/oSZ7Y0/w/9utafL8T+rWmfxyXo2A6kceixN/qsz+UYo5MO11Tm3\nuIXfW4QfEqmpC/6MuSEz8cMQ1wCr8X9RfALs1tA3NdPfkts918xeBQYBp7VxHdWHHLbX0daS4coK\nwGo8l1/j66D21RI1p8s1t5aNQNcaz1W+XzOn2nOHAQudc2/VtREzux24veFSOd0592Yj69SnW/Lf\nDS38/qyhkM9cC4EzzMxc8u/XpGOSbXUys+7A14ERzrnXk88dQ9Wx8ClQBpwC/KeezZTjhwfq5Zwr\nM7Nn8WfwPYAv8MMHTa2jSfvBD1GUA99JPib5hu9xwJRGvrclNuDHyas7CljWxO8Pon9T+Zo/wA/5\nVdcF/59DIrmvjvix+C8a2M4fgKmN7Gt1y0oE4BvAaufcukbXzHIK+XDtnvxTuLqEc67y7KSTmfWr\n0R53zi0DJuDfSHvIzP6IH+c9Az/j4JwG9vkl/mztajNbCewH/Bp/Fo1z7isz+w1wj5mV4YeUugP9\nnXMTkttYhn/Tqzd+3HSzc66umRBP4oclvgb8pcY6DdbR1P0450rMbAJwr5ltBJYCo4EC4PcN9ENL\nvQaMN7Nz8P+ZXgPsTxNDvqX9W2MbqXzN/0hut7tzblPyuXn4v15uM7On8D+ntcDBZnaIc67Wf1Yt\nHa5JvkF/cPLLHPzspn74n/2KaquemKxVGhP2mwLZuuDfSHN1LKsaaX+u2ja+hT/Q1+GHaGYD5zVh\n3yfj5yJvS/57GtXe5ML/ct2K/wBQOX52x6+qff+h+BkgW5M19a5W88xq6xk+sBxwZAvqaOp+dsfP\n0lmHP0t+j+Sbt8n2GA28kdlAP9X1xms+fm72xuTyC2q/8drgvlrSv819za183e8CI2s8dzv+r5ht\nwFP4IZ23gQ0B/14UUvdxP6naOnvgj/f/Cvv3OBMWfeJVRHZhZoOB3wB9nXOJsOupycxGAuc6504N\nu5ZMoHnyIrIL59zf8X+t9Aq7lnpsB34SdhGZQmfyIiIRpjN5EZEIU8iLiERY6FMoe/To4Xr37h1q\nDSUlJbRv3z7UGtKF+sJbuHAhiUSCvn1rfoA0O6XrcVFWBp9+CokEFBRArzZ4FyFd+mLu3LkbnXM9\nG1sv9JDv3bs3c+bMaXzFFIrFYhQWFoZaQ7pQX3iFhYXE4/HQj810kY7HxZYtcNxxPuDPPBNefBFy\nG/voXADSpS/MbHlT1tNwjYhknEQCLr3Un8UfcQRMmdI2AZ+JFPIiknFuvhlefhm6d4fp06FTp7Ar\nSl8KeRHJKI8+Cg8+CPn58Pzz0KdP2BWlt0BD3syeNLMvzKzIzBaZ2Y+D3L6IZLdZs2D4cP94wgQ4\n6aRw68kEQZ/J/z+gj3OuE/4iWWPNrH/A+xCRLLR0KVxwAWzfDqNHw49+FHZFmSHQkHfOzXf+BhFQ\ndWGhg4Lch4hkn6IiOPts2LQJBg+G++4Lu6LMEfgUSjP7Pf561Hvir039Uh3rDCN5R6SCggJisVjQ\nZTRLcXFx6DWkC/WFF4/HSSQS6oukMI+LRALGjPkmCxZ058ADSxg58t+89VZ4103LuN+RVFzaEn/D\ngxOAMUB+Q+v279/fhe31118Pu4S0ob7wBg4c6I466qiwy0gbYR4XN93kHDjXrZtzixeHVsZO6fI7\nAsxxTcjjlMyucc4lnL8tWC9geCr2ISLRN2kS3H8/5OXBtGlwkAZ/my3VUyjz0Ji8iLTAW2/BsORt\n7h9+GNLgQ6YZKbCQN7O9zOz7ZtbBzHLN7DT8rehac1d6EclCy5bB+ef7mTTXX18V9tJ8Qb7x6vBD\nM3/A/+exHBjlnJse4D5EJOK++grOOQc2boRTT4UHHgi7oswWWMg7f/PpgUFtT0SyT0UFXHEFfPwx\nHHYYPPOMH4+XltNlDUQkbdx+u78WTdeuMGMGdOkSdkWZTyEvImnhiSfg3nv91SSfew4OOSTsiqJB\nIS8ioXvnHbj6av/4oYfg5JPDrSdKFPIiEqrly/1MmvJyGDmy6gJkEgyFvIiEprjYz6RZvx4GDYLx\n48OuKHoU8iISiooKGDIEPvrIj79PnaqZNKmgkBeRUNxxB7zwgp9BM2OGn1EjwVPIi0ibe+opGDfO\nz6SZOtXPiZfUUMiLSJuaPbvqhh/jx8P3vhduPVGnkBeRNrNyJZx7LpSVwbXX+tk0kloKeRFpEyUl\nfibNunV+HvxvfwtmYVcVfQp5EUm5igr4wQ9g3jw4+GB49lnIzw+7quygkBeRlLvrLnj+eejc2c+k\n6dYt7Iqyh0JeRFLq6afh7rshJ8c//vrXw64ouyjkRSRl3n8frrzSP/7f/4XBg8OtJxsp5EUkJVav\nhvPOg23b/MXHrr8+7Iqyk0JeRAK3daufKrl2LQwcCL/7nWbShEUhLyKBqqiAoUNh7lzo0wemTYPd\ndgu7quylkBeRQN19t58i2amTn0nTvXvYFWU3hbyIBObZZ/10ycqZNH37hl2RKORFJBBz58IPf+gf\n//rXcPrp4dYjnkJeRFptzRp/yYLSUrjqKhg9OuyKpJJCXkRapbTUT5VcswZOPBEmTNBMmnSikBeR\nFnPOn7n/61/Qu7dm0qQjhbyItNjYsf4N1g4d/Eyanj3DrkhqUsiLSItMmwZ33umHZv7yF/jGN8Ku\nSOqikBeRZvvgA3/pYID77oOzzgq3HqmfQl5EmmXtWj+TZutWP2XyxhvDrkgaopAXkSbbtg3OPx9W\nrYLvfAceeUQzadKdQl5EmsQ5fwPu2bPhwAP9TUB23z3sqqQxgYW8me1uZo+a2XIz+8rM5pmZPvMm\nEhH33ANTpkD79jB9Ouy1V9gVSVMEeSafB6wEBgKdgTHAVDPrHeA+RCQEb77Zg5//3A/NTJkCRx4Z\ndkXSVHlBbcg5VwLcVe2pmWa2FOgPLAtqPyLStubNg3HjDgf82fw554RckDRLysbkzawAOBRYkKp9\niEhqrVvnQ33btlyGDIFbbgm7ImmuwM7kqzOzfOAp4HHn3Gd1tA8DhgEUFBQQi8VSUUaTFRcXh15D\nulBfePF4nEQikdV9UV6ew09/ehQrV3bmsMO+5IorPuaNNyrCLit0mfY7EnjIm1kOMBkoB66rax3n\n3ERgIsCAAQNcYWFh0GU0SywWI+wa0oX6wuvSpQvxeDxr+8I5Pwd+wQLYf38YN+4TTj31pLDLSguZ\n9jsSaMibmQGPAgXAGc657UFuX0Taxn33weTJ0K6dn0kTj+tXOVMFPSY/ATgcONs5VxrwtkWkDUyf\nDrfd5h8/+ST06xduPdI6Qc6TPxC4BugHfGFmxcnl8qD2ISKp9dFHcNllfrjmV7/yn26VzBbkFMrl\ngD7gLJKh1q/3M2lKSnzQV57NS2bTZQ1EhLIyuOACWL4cjj0W/vQnXZMmKhTyIlnOObj2Wnj7bejV\nC154AfbcM+yqJCgKeZEs98ADMGmSD/YXX4R99gm7IgmSQl4ki82cWfUp1smT4Zhjwq1HgqeQF8lS\n8+fDpZf64Zpf/hIuvDDsiiQVFPIiWWjDBjj7bCguhu9/H8aMCbsiSRWFvEiWKS/3Z+3LlsGAAfDY\nY5pJE2UKeZEs4hyMGAFvvgn77uvfaNVMmmhTyItkkfHj4dFHq2bS7Ltv2BVJqinkRbLEyy/DTTf5\nx5Mm+aEaiT6FvEgW+OQT/wZrRQX8z//AxReHXZG0FYW8SMRt3Ohn0hQVwX//N9x5Z9gVSVtSyItE\nWHk5XHQRfP459O/vh2ly9FufVfTjFoko5+AnP4E33vCXKnjxRX8TEMkuCnmRiHroIZg4EfbYw190\nbL/9wq5IwqCQF4mgf/wDRo/2jx97zF8+WLKTQl4kYj77DC65xM+kGTPGX59GspdCXiRCNm/2M2m2\nbPGXLvjFL8KuSMKmkBeJiO3b/RTJxYvh6KPh8cc1k0YU8iKRccMN8NprUFDgZ9K0bx92RZIOFPIi\nEfDwwzBhAuy+uw/4/fcPuyJJFwp5kQz3z3/6s3jwFx/79rfDrUfSi0JeJIMtWuSvQ5NIwG23weWX\nh12RpBuFvEiG+vJLP5MmHofzzoOxY8OuSNKRQl4kA23f7s/gFy2Co47yN+HWTBqpiw4LkQw0ejS8\n8grstRdMnw4dOoRdkaQrhbxIhpkwwc+m2W03f02aAw4IuyJJZwp5kQzy2mv+ypIAf/wjHHdcuPVI\n+lPIi2SI//zHXxs+kYBbboEf/CDsiiQTKORFMkA87mfSVM6oGTcu7IokUwQa8mZ2nZnNMbMyM5sU\n5LZFstWOHf6qkgsXwje/CU89Bbm5YVclmSIv4O2tAcYCpwF7Brxtkax0443wf/8HPXv6mTQdO4Zd\nkWSSQEPeOfc8gJkNAHoFuW2RbDRxIvz2t5CfD88/D717h12RZBqNyYukqVgMRo70jydOhBNOCLUc\nyVBBD9c0iZkNA4YBFBQUEIvFwihjp+Li4tBrSBfqCy8ej5NIJELri9Wr92DEiP7s2JHPxRevpHfv\nJYT5Y9FxUSXT+iKUkHfOTQQmAgwYMMAVFhaGUcZOsViMsGtIF+oLr0uXLsTj8VD6YssWGDECiorg\nzDNhypT9yc0N99rBOi6qZFpfaLhGJI0kEv6erJ9+CkccAVOmaCaNtE6gZ/JmlpfcZi6Qa2Z7ADuc\nczuC3I9IVN18M7z8MnTv7mfSdOoUdkWS6YI+kx8DlAK3AlckH48JeB8ikfToo/Dgg1Uzafr0Cbsi\niYKgp1DeBdwV5DZFssGsWTB8uH88YQKcdFK49Uh0aExeJGRLl8IFF/hrxI8eDT/6UdgVSZQo5EVC\nVFTkr0WzaRMMHgz33Rd2RRI1CnmRkCQScNllsGABHH44PP005IUyqVmiTCEvEpJbb4W//Q26dYMZ\nM6Bz57ArkihSyIuEYNIkuP9+f+Y+bRocdFDYFUlUKeRF2thbb8GwYf7xww9DBn14UjKQQl6kDS1b\nVjWT5vrrq8JeJFUU8iJt5Kuv/EyaDRvg1FPhgQfCrkiygUJepA0kEnD55TB/Phx2GDzzjGbSSNtQ\nyIu0gZ//3M+g6drV/9ulS9gVSbZQyIuk2BNPwL33+qtJPvccHHJI2BVJNlHIi6TQO+/A1Vf7xw89\nBCefHG49kn0U8iIpsnw5nH8+lJf72/hVXoBMpC0p5EVSoLgYzjkH1q+HQYNg/PiwK5JspZAXCVhF\nBQwZAh99BIceClOnaiaNhEchLxKwMWPghRf8DJrKGTUiYVHIiwToySfhnnv8TJpnn/Vn8iJhUsiL\nBOS99+DHP/aPf/MbPxYvEjaFvEgAVqyA886DsjI/i2bkyLArEvEU8iKtVFIC554L69bBKaf4s3iR\ndKGQF2mFypk08+bBwQf7mTT5+WFXJVJFIS/SCnfeCX/9q7+r04wZ/i5PIulEIS/SQlOmwK9+5WfS\nTJ0KX/962BWJ1KaQF2mB2bPhqqv84wcf9NeHF0lHCnmRZlq5smomzTXXwHXXhV2RSP0U8iLNUDmT\n5osv/L1ZH3oIzMKuSqR+CnmRJqqogB/+ED74AA46yF8bXjNpJN0p5EWa6K67YNo06NTJz6Tp3j3s\nikQap5AXaYKnn4a774acHH9/1sMPD7sikaZRyIs04v334cor/eMHHoDBg8OtR6Q5FPIiDVi92s+k\n2bbNX3zshhvCrkikeQINeTPrZmZ/NbMSM1tuZpcFuX2RtlRRYZx7LqxdCwMHwsMPayaNZJ6g71fz\nMFAOFAD9gL+Z2YfOuQUB70ck5VasaMeWLdCnj59Js9tuYVck0nzmnAtmQ2btgS+BbzjnFiWfewJY\n45y7tb7v69ixo+vfv38gNbRUPB6nS5cuodaQLtQX3nvvzaOsDHJz+3H00dC+fdgVhUvHRZV06Ys3\n3nhjrnNuQGPrBXkmfyiwozLgkz4ECmuuaGbDgGEA+fn5xOPxAMtovkQiEXoN6UJ9AfF4PmVl/vEB\nB5Swfft2srxLdFxUk2l9EWTIdwCKajxXBHSsuaJzbiIwEWDAgAFuzpw5AZbRfLFYjMLCwlBrSBfZ\n3hevv145e6aQffct5fPPZ4ddUlrI9uOiunTpC2viG0RBhnwx0KnGc52BrwLch0jKfPSRn0lTXg77\n7Qc9epSFXZJIqwU5u2YRkGdmh1R77ihAb7pK2lu+3J/BFxXBRRf5yxaIREFgIe+cKwGeB35pZu3N\n7ATgHGByUPsQSYUvvoDTTquaKjl5sqZKSnQE/WGoEcCewHpgCjBc0yclna1bByefDAsXwpFHwgsv\nwB57hF2VSHACnSfvnNsMnBfkNkVSZf16f+PtTz+Fb3wDXnkF0mBmnEigdFkDyUqVAb9gAfTtC6++\nCj17hl2VSPAU8pJ1li6F73wH5s/3V5N87TXYa6+wqxJJDYW8ZJWPPoLjj4fFi+Hoo/28+IKCsKsS\nSR2FvGSNN96Ak07ys2m++12IxRTwEn0KeckKf/oTfO97sGULXHghvPSSv8OTSNQp5CXSduyAUaPg\n6qth+3YYPdrf2UnTJCVbBH2pYZG0sWEDXH45/POf/obbf/gDXHVV2FWJtC2FvETSrFlw6aWwZo2f\nGvnXv/oZNSLZRsM1EimJBIwd699YXbMGTjgB/v1vBbxkL4W8RMbixf7aM3fcARUVcNttfopkr15h\nVyYSHg3XSMarqPD3X/3Zz6C0FPbeGyZN8hcdE8l2CnnJaAsWwPDh8Oab/uvLLoOHHoJu3cKtSyRd\naLhGMlJxMdxyC/Tr5wO+Z0+YNg2eekoBL1KdQl4ySkWFv9774YfDr3/t32i99lp/qeALLgi7OpH0\no+EayRivvgo33wwffOC/PuYYmDABjj023LpE0pnO5CXtvfWWvyTBoEE+4Pfbz7+x+v77CniRxuhM\nXtKSc/4DTWPH+pt5gL/WzM9+5i9T0K5duPWJZAqFvKSV8nJ/bZnx4/2HmMCH+6hRfunaNdz6RDKN\nQl7SwsaN8Mgjfr772rX+uZ49YcQIuOEGhbtISynkJTQ7dsA//gF//jNMn+6vEgn+fqujRvmLi+lq\nkSKto5CXNuUcfPIJPPGEnwpZedaekwNnnunD/ZRTwCzcOkWiQiEvKecczJvnP6w0bRp89llV26GH\nwpVXwpAhftaMiARLIS8pUVbmP4n697/7y/x+/nlVW7du/oNLV14Jxx2ns3aRVFLISyCcg0WL/A06\n/v53f/XHrVur2vfaC84/Hy66yF8pMj8/vFpFsolCXlokkYD58/1c9lmz/Fn7unW7rnPkkTB4MJxx\nhr+ue25uOLWKZDOFvDTKOViyBObOhTlz/DJ3Lnz11a7r7bWXv1nH4MFw6qmw777h1CsiVRTysouS\nklzee8/PgFmwAD780Ad6PF573QMO8EMvJ53kl0MO0fi6SLpRyGehsjJYtgyWLvVviC5eXBXqq1ad\nWOf3FBTAt74FAwZA//5+2Weftq1bRJpPIR8xzsGWLf7+pmvWwOrVsHx5VaB//rl/zrm6vz8/v4K+\nfXM44gjo29d/MGnAAD/0orN0kcyjkM8AiQR8+aX/6H/NZcMG/4Gi1aurgr36rJa65Ob6oZY+faqW\nvn39snz5LE45pbBNXpeIpF4gIW9m1wFDgW8Cf3HODQ1iu1GwY4e/7+jWrVBU5JctWxr+t6jIj4Fv\n2uSDfPPm+s+869K+vf9g0b77+mX//eGgg3yYf+1r/uv6pjCuWhXM6xaR9BDUmfwaYCxwGrBnQNts\nsooKH6aJRN3Ljh3+6oZ1Ldu3w5w53fjyy/rXqVyvrKwqsCv/re9x5b+V12Npra5doUePupd99vFh\nXhnsHTtqaEVEvEBC3jn3PICZDQB6Ned7P/hgIR06FOJc1dlqu3YX06HDCLZv38rGjWfsbKtccnOH\nYjaUHTs2UlFxUR1bHQ5cAqwEhtTRfiNwNrAQuKaO9jHAIGAeMKqO9nHA8cA7wO11tI8H+gGvAGPJ\nyfFDJLm5kJcHffs+wt57H0ZR0QwWLXqAvLyqtrw8uPnmyRx88P68//4zPP/8BPLydg3txx57jh49\nejBp0iQmTZpUa+8vvfQS7dq14/e//z1Tp06t1R6LxQC4//77mTlz5i5tpaWlzJ49G4C7776bV199\ndZf27t27M23aNABuu+023n333V3ae/XqxZNPPgnAqFGjmDdv3i7thx56KBMnTgRg2LBhLFq0aJf2\nfv36MX78eACuuOIKVtX40+K4447jnnvuAeDCCy9k06ZNu7Sfcsop3HHHHQCcfvrplJaW7tJ+1lln\ncdNNNwFQWFhITRdffDEjRoygoqKCxYsX11pn6NChDB06lI0bN3LRRbWPveHDh3PJJZewcuVKhgyp\nfezdeOONnH322SxcuJBrrql97I0ZM4ZBgwYxb948Ro2qfeyNGzeO448/nnfeeYfbb6997I0fP55+\n/frxyiuvMHbs2FrtjzzyCIcddhgzZszggQceqNU+efJk9t9/f5555hkmTJiw8/l4PE6XLl147rnU\nHXt77rknL7/8MpDdx97WrVs544wzarU3duzVJ5QxeTMbBgzzX3WgpGTX9tJSP1RRn4qK+rYL4MjP\nT7DbbtuB7Wzb5gBHTo5vN3N07VpK165b2LGjiDVrdgAV5OQYZpCT4zj44E3svfcaiovXMX9+GWYu\n+b2+/cQTl9OnT1fWrfuc118vISfHJRe//auumsfhhxezYMGH/OUvtecejhw5mwMOWMs773zMl1/W\nbm/f/l0SiSVs2bKAkpLa7W+//TadO3fms88+I17H3MZZs2axxx57sGjRojrbK3/RlixZUqs9Nzd3\nZ/vSpUtrtVdUVOxsX7FiRa32/Pz8ne2rVq2q1b5mzZqd7WvWrKnVvmrVqp3t69atq9W+YsWKne0b\nNmygqKhol/alS5fubN+8eTNlZWW7tC9ZsmRne119s2jRImKxGPF4HOdcrXU+++wzYrEYW7ZsqfP7\nFyxYQCwWY/369XW2f/zxx3Ts2LHOvgP48MMPycvLY/HixXW2//vf/6a8vJz58+fX2T5nzhzi8Tgf\nfvhhne2zZ89m7dq1fPzxx3W2v/vuuyxZsoQFCxbs0p5IJIjH4yk99kpLSzPi2CsuLk7psbdt27Y6\n2xs79upjrjmDvY1tzGws0Ks5Y/J9+w5wU6bM2XmmW9dSeaZb35LTypsYxmKxOv9nzUbqC6+wsJB4\nPF7rbDBb6bioki59YWZznXMDGluv0TN5M4sBA+tpfts5d0Iza9tFu3bQr19rtiAiIvVpNOSdc4Vt\nUIeIiKRAUFMo85LbygVyzWwPYIdzbkcQ2xcRkZZp5Wj2TmOAUuBW4Irk4zEBbVtERFooqCmUdwF3\nBbEtEREJTlBn8iIikoYU8iIiEaaQFxGJMIW8iEiEKeRFRCJMIS8iEmEKeRGRCFPIi4hEmEJeRCTC\nFPIiIhGmkBcRiTCFvIhIhCnkRUQiTCEvIhJhCnkRkQhTyIuIRJhCXkQkwhTyIiIRppAXEYkwhbyI\nSIQp5EVEIkwhLyISYQp5EZEIU8iLiESYQl5EJMIU8iIiEaaQFxGJMIW8iEiEKeRFRCJMIS8iEmGt\nDnkz293MHjWz5Wb2lZnNM7PTgyhORERaJ4gz+TxgJTAQ6AyMAaaaWe8Ati0iIq2Q19oNOOdKgLuq\nPTXTzJavAb8lAAADa0lEQVQC/YFlrd2+iIi0XOBj8mZWABwKLAh62yIi0jytPpOvzszygaeAx51z\nnzWw3jBgGEBBQQGxWCzIMpqtuLg49BrShfrC69GjB507d1ZfJOm4qJJpfWHOuYZXMIvhx9vr8rZz\n7oTkejnAFKATcK5zbntTChgwYICbM2dOkwtOhVgsRmFhYag1pAv1RRX1RRX1RZV06Qszm+ucG9DY\neo2eyTvnCpuwMwMeBQqAM5oa8CIiklpBDddMAA4HBjnnSgPapoiItFIQ8+QPBK4B+gFfmFlxcrm8\n1dWJiEirBDGFcjlgAdQiIiIB02UNREQiTCEvIhJhjU6hTHkBZhuA5aEWAT2AjSHXkC7UF1XUF1XU\nF1XSpS8OdM71bGyl0EM+HZjZnKbMN80G6osq6osq6osqmdYXGq4REYkwhbyISIQp5L2JYReQRtQX\nVdQXVdQXVTKqLzQmLyISYTqTFxGJMIW8iEiEKeTrYGaHmNk2M3sy7FrCkO337TWzbmb2VzMrSfbB\nZWHXFIZsPw7qk2n5oJCv28PAv8IuIkTZft/eh4Fy/KWzLwcmmNkR4ZYUimw/DuqTUfmgkK/BzL4P\nxIFXw64lLM65EufcXc65Zc65CufcTKDyvr2RZmbtgQuBO5xzxc65t4AXgSHhVtb2svk4qE8m5oNC\nvhoz6wT8Evhp2LWkkyy7b++hwA7n3KJqz30IZOOZ/C6y7DioJVPzQSG/q7uBR51zq8IuJF009b69\nEdIBKKrxXBHQMYRa0kYWHgd1ych8yJqQN7OYmbl6lrfMrB8wCHgw7FpTrbG+qLZeDjAZPz59XWgF\nt61i/H2Kq+sMfBVCLWkhS4+DXWRyPgR1+7+019i9as1sFNAbWOFvWUsHINfM+jrnjkl5gW1I9+1t\n0CIgz8wOcc79J/ncUWTvEEW2Hgc1FZKh+aBPvCaZWTt2PYO7Cf9DHe6c2xBKUSEysz/gb+k4yDlX\nHHY9bcnMngYc8GPgaOBvwPHOuawL+mw+DqrL5HzImjP5xjjntgJbK782s2JgW7r/AFOh2n17y/D3\n7a1susY591RohbWdEcBjwHpgE/4XORsDPtuPg50yOR90Ji8iEmFZ88ariEg2UsiLiESYQl5EJMIU\n8iIiEaaQFxGJMIW8iEiEKeRFRCJMIS8iEmEKeRGRCPv/MbG5kKbPx0UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ff01722e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elu(z,alpha=1):\n",
    "    return np.where(z<0,np.exp(z)*alpha-1,z)\n",
    "\n",
    "z=np.linspace(-5,5,200)\n",
    "plt.plot(z,elu(z),'b-',linewidth=2)\n",
    "plt.plot([-5,5],[0,0],'k-')\n",
    "plt.plot([-5,5],[-1,-1],'k--')\n",
    "plt.plot([0,0],[-2,5],'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1=tf.layers.dense(X,300,activation=tf.nn.elu,name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SELU\n",
    "*这个激活功能是由（https://arxiv.org/pdf/1706.02515.pdf）中提出的。\n",
    "在训练期间，一个神经网络专门由使用SELU激活函数和LeCun初始化的一堆密集层将自我标准化：每层的输出将倾向于在训练期间保持相同的均值和方差，\n",
    "这解决了消失/爆炸的梯度问题。因此，这种激活功能对于这样的神经网络来说非常显着地优于其他激活功能，\n",
    "遗憾的是，SELU激活函数的自标准化特性很容易被破坏：你不能使用ℓ1或ℓ2 正则化， regular dropout, max-norm,跳过连接或其他非连续拓扑\n",
    "（因此递归神经网络不会自我标准化）。但是，在实践中，它对顺序CNN非常有效。如果打破自我规范化，SELU不一定会胜过其他激活函数。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67326324235\n",
      "1.05070098736\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
    "print(alpha_0_1)\n",
    "print(scale_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.03885500718\n",
      "-0.9932620530009145\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEMCAYAAAAh7MZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7P8BcPicYke4XzcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12fee7b30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):#官方文档参数写反了\n",
    "    return scale * elu(z, alpha)\n",
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "print(selu(-5))\n",
    "print(elu(-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，SELU超参数（`scale`和`alpha`）的调整方式使得每个神经元的平均输出保持接近0，标准偏差保持接近1（假设输入标准化为均值0 和标准差1）。 使用此激活功能，即使是1,000层深度神经网络也可以在所有层上保留大致平均值0和标准偏差1，从而避免爆炸/消失梯度问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean 0.35, std deviation 0.73\n",
      "Layer 100: mean 0.31, std deviation 0.21\n",
      "Layer 200: mean 0.29, std deviation 0.20\n",
      "Layer 300: mean 0.29, std deviation 0.21\n",
      "Layer 400: mean 0.30, std deviation 0.21\n",
      "Layer 500: mean 0.29, std deviation 0.21\n",
      "Layer 600: mean 0.34, std deviation 0.21\n",
      "Layer 700: mean 0.33, std deviation 0.21\n",
      "Layer 800: mean 0.26, std deviation 0.21\n",
      "Layer 900: mean 0.31, std deviation 0.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实就是ELU乘了个lambda，关键在于这个lambda是大于1的。以前relu，prelu，elu这些激活函数，都是在负半轴坡度平缓，这样在activation的方差过大的时候可以让它减小，防止了梯度爆炸，但是正半轴坡度简单的设成了1。而selu的正半轴大于1，在方差过小的的时候可以让它增大，同时防止了梯度消失。这样激活函数就有一个不动点，网络深了以后每一层的输出都是均值为0方差为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def selu(z, scale=alpha_0_1, alpha=scale_0_1):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not forget to scale the inputs to mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.94 Validation accuracy: 0.9314\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.961\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9674\n",
      "15 Batch accuracy: 0.96 Validation accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "Note:现在我们使用 `tensorflow.contrib.layers.batch_norm()`，不用 `tf.layers.batch_normalization()`，后者已经删除.\n",
    "更流行的是使用 `tf.layers.batch_normalization()`,因为contrib模块中的任何内容都可能会改变或被删除，恕不另行通知。 我们现在不使用batch_norm()函数作为fully_connected()函数的正则化参数，而是使用batch_normalization()，并明确地创建一个不同的层。 参数有些不同，特别是：\n",
    "* `decay`变成 `momentum`,\n",
    "* `is_training`编程 `training`,\n",
    "* `updates_collections` is removed: batch normalization所需的更新操作被添加到`UPDATE_OPS`集合中，你需要在训练期间明确地运行这些操作  (see the execution phase below),\n",
    "* 我们不需要指定scale = True，因为这是默认值\n",
    "\n",
    "还要注意，为了在每个隐藏层激活函数之前运行批量标准化，我们要在批量规范层之后，手动应用 RELU 激活函数。\n",
    "注意：由于tf.layers.dense()函数与本书中使用的tf.contrib.layers.arg_scope()不兼容，我们现在使用 python 的functools.partial()函数。 它可以很容易地创建一个my_dense_layer()函数，只需调用tf.layers.dense()，并自动设置所需的参数（除非在调用my_dense_layer()时覆盖它们）。 如您所见，代码保持非常相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免一遍又一遍重复相同的参数，我们可以使用 Python 的partial()函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')#给Batch norm加一个placeholder\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  using the ELU activation function and Batch Normalization ，build a neural net for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "batch_norm_momentum=0.9\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_inputs),name=\"X\")\n",
    "y=tf.placeholder(tf.int32,shape=(None),name=\"y\")\n",
    "training=tf.placeholder_with_default(False,shape=(),name=\"training\")#存的是bool类型，\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init=tf.variance_scaling_initializer()\n",
    "    my_batch_norm_layer=partial(tf.layers.batch_normalization,training=training,momentum=batch_norm_momentum)\n",
    "    my_dense_layer=partial(tf.layers.dense,kernel_initializer=he_init)\n",
    "    \n",
    "    hidden1=my_dense_layer(X,n_hidden1,name=\"hidden1\")\n",
    "    bn1=tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2=my_dense_layer(bn1,n_hidden2,name=\"hidden2\")\n",
    "    bn2=tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn=my_dense_layer(bn2,n_outputs,name=\"outputs\")\n",
    "    logits=my_batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss=tf.reduce_mean(xentropy,name=\"Loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct=tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    \n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8952\n",
      "1 Validation accuracy: 0.9202\n",
      "2 Validation accuracy: 0.9318\n",
      "3 Validation accuracy: 0.9422\n",
      "4 Validation accuracy: 0.9468\n",
      "5 Validation accuracy: 0.954\n",
      "6 Validation accuracy: 0.9568\n",
      "7 Validation accuracy: 0.96\n",
      "8 Validation accuracy: 0.962\n",
      "9 Validation accuracy: 0.9638\n",
      "10 Validation accuracy: 0.9662\n",
      "11 Validation accuracy: 0.9682\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9704\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9726\n",
      "18 Validation accuracy: 0.9738\n",
      "19 Validation accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "#注意：由于我们使用的是 tf.layers.batch_normalization() 而不是 tf.contrib.layers.batch_norm()（如本书所述），\n",
    "# 所以我们需要明确运行批量规范化所需的额外更新操作（sess.run([ training_op，extra_update_ops], ...)。\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   这对 MNIST 来说不是一个很好的准确性。 当然，如果你训练的时间越长，准确性就越好，但是由于这样一个浅的网络，批量归一化和 ELU 不太可能产生非常积极的影响：它们大部分都是为了更深的网络而发光。请注意，您还可以训练操作取决于更新操作：\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "This way, you would just have to evaluate the `training_op` during training, TensorFlow would automatically run the update operations as well:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping梯度裁剪\n",
    "_Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we apply gradient clipping. For this, we need to get the gradients, use the clip_by_value() function to clip them, then apply them:\n",
    "learning_rate=0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.288\n",
      "1 Validation accuracy: 0.7936\n",
      "2 Validation accuracy: 0.8796\n",
      "3 Validation accuracy: 0.9056\n",
      "4 Validation accuracy: 0.9164\n",
      "5 Validation accuracy: 0.9214\n",
      "6 Validation accuracy: 0.9294\n",
      "7 Validation accuracy: 0.9358\n",
      "8 Validation accuracy: 0.9376\n",
      "9 Validation accuracy: 0.9412\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "n_epochs=10\n",
    "batch_size=200\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9458\n",
      "1 Validation accuracy: 0.947\n",
      "2 Validation accuracy: 0.9476\n",
      "3 Validation accuracy: 0.9532\n",
      "4 Validation accuracy: 0.9564\n",
      "5 Validation accuracy: 0.9566\n",
      "6 Validation accuracy: 0.9576\n",
      "7 Validation accuracy: 0.9586\n",
      "8 Validation accuracy: 0.9626\n",
      "9 Validation accuracy: 0.961\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，一般情况下，您只需要重新使用原始模型的一部分（就像我们将要讨论的那样）。如果您使用import_meta_graph（），它将加载整个图形，但您可以简单地忽略您不需要的部分。 在这个例子中，我们在预训练的第3层顶部添加了一个新的第4个隐藏层（忽略旧的第4个隐藏层）。 我们还构建了一个新的输出层，这个新输出的损失，以及一个新的优化器来最小化它。 我们还需要另一个保存程序来保存整个图形（包含整个旧图形和新操作），以及初始化操作以初始化所有新变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "#Next you need to get a handle on all the operations you will need for training. \n",
    "#If you don't know the graph's structure, you can list all the operations:\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8972\n",
      "1 Validation accuracy: 0.9172\n",
      "2 Validation accuracy: 0.928\n",
      "3 Validation accuracy: 0.9342\n",
      "4 Validation accuracy: 0.939\n",
      "5 Validation accuracy: 0.94\n",
      "6 Validation accuracy: 0.946\n",
      "7 Validation accuracy: 0.9508\n",
      "8 Validation accuracy: 0.9512\n",
      "9 Validation accuracy: 0.9526\n"
     ]
    }
   ],
   "source": [
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "\n",
    "#执行\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")#加载旧模型\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")#保存新模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你可以访问构建原始图形的Python代码，则可以重用所需的部分并删除其余部分：（上面的情况是只有原模型文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.892\n",
      "1 Validation accuracy: 0.9174\n",
      "2 Validation accuracy: 0.9292\n",
      "3 Validation accuracy: 0.9366\n",
      "4 Validation accuracy: 0.9394\n",
      "5 Validation accuracy: 0.943\n",
      "6 Validation accuracy: 0.9436\n",
      "7 Validation accuracy: 0.947\n",
      "8 Validation accuracy: 0.9484\n",
      "9 Validation accuracy: 0.951\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "#创建一个restore_saver来恢复预训练模型（为其提供要恢复的变量列表，否则它会抱怨图形不匹配）    \n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                            # not shown in the book\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.883\n",
      "1 Validation accuracy: 0.9122\n",
      "2 Validation accuracy: 0.9232\n",
      "3 Validation accuracy: 0.9294\n",
      "4 Validation accuracy: 0.9324\n",
      "5 Validation accuracy: 0.934\n",
      "6 Validation accuracy: 0.9352\n",
      "7 Validation accuracy: 0.9356\n",
      "8 Validation accuracy: 0.9362\n",
      "9 Validation accuracy: 0.9366\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,#在训练期间冻结较低层，最简单的解决方案是给优化器列出要训练的变量，不包括来自较低层的变量：\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    \n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") \n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum optimization：\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)\n",
    "                                       \n",
    "Nesterov Accelerated Gradient：\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)\n",
    "                                       \n",
    "AdaGrad：\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "RMSProp：\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)\n",
    "                                      \n",
    "Adam Optimization：\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9626\n",
      "1 Validation accuracy: 0.9722\n",
      "2 Validation accuracy: 0.9746\n",
      "3 Validation accuracy: 0.9806\n",
      "4 Validation accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization\n",
    "\n",
    "## $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "\n",
    "#将正则项添加到损失函数中\n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")#Returns the `Tensor` with the given `name`.\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者，我们可以将正则化函数传递给tf.layers.dense（）函数，该函数将使用它来创建计算正则化损失的运算，并将这些运算添加到正则化损失的集合中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "scale=0.001\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))        #添加l1正则函数\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8274\n",
      "1 Validation accuracy: 0.8766\n",
      "2 Validation accuracy: 0.8952\n",
      "3 Validation accuracy: 0.9016\n",
      "4 Validation accuracy: 0.908\n",
      "5 Validation accuracy: 0.9096\n",
      "6 Validation accuracy: 0.9124\n",
      "7 Validation accuracy: 0.9154\n",
      "8 Validation accuracy: 0.9178\n",
      "9 Validation accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "#将正则loss添加到loss\n",
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')#训练时True，测试时False\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clip_norm >= l2_norm\n",
    "#           t = t\n",
    "#    else\n",
    "#          t = t*clip_norm/l2norm(t)\n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs=10\n",
    "batch_size=50\n",
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)                 # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  另一种方法\n",
    "定义一个max_norm_regularizer()函数，用这个函数来得到一个最大范数调节器（与你想要的阈值）。\n",
    "就是调用这个函数会返回一个函数。 当你创建一个隐藏层时，你可以将这个正则化器传递给kernel_regularizer参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)            \n",
    "        tf.add_to_collection(collection, clip_weights)                   #把变量放入一个集合\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9556\n",
      "1 Validation accuracy: 0.9704\n",
      "2 Validation accuracy: 0.9672\n",
      "3 Validation accuracy: 0.9734\n",
      "4 Validation accuracy: 0.977\n",
      "5 Validation accuracy: 0.9752\n",
      "6 Validation accuracy: 0.9804\n",
      "7 Validation accuracy: 0.9802\n",
      "8 Validation accuracy: 0.9836\n",
      "9 Validation accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)                           # 得到一个最大范数调节器\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs=10\n",
    "batch_size=50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")       #把裁剪的权重取出来，从一个集合中取出全部变量，是一个列表\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
